# ğŸ¤– Sonnetì„ ìœ„í•œ AI Visualizer ê°œë°œ ê°€ì´ë“œ

> **Claude Sonnetì´ ì´ ë¬¸ì„œë§Œ ì½ê³  ê°œë°œì„ ì‹œì‘í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.**

---

## ğŸ“‹ ëª©ì°¨

1. [ì‹œì‘í•˜ê¸° ì „ì—](#ì‹œì‘í•˜ê¸°-ì „ì—)
2. [Phase 1: í”„ë¡œì íŠ¸ ìƒì„±](#phase-1-í”„ë¡œì íŠ¸-ìƒì„±)
3. [Phase 2: Database ê°œë°œ](#phase-2-database-ê°œë°œ)
4. [Phase 3: Backend API ê°œë°œ](#phase-3-backend-api-ê°œë°œ)
5. [Phase 4: Frontend ê¸°ë³¸ ê°œë°œ](#phase-4-frontend-ê¸°ë³¸-ê°œë°œ)
6. [Phase 5: ì‹œê°í™” êµ¬í˜„](#phase-5-ì‹œê°í™”-êµ¬í˜„)

---

## ì‹œì‘í•˜ê¸° ì „ì—

### âœ… ì½ì–´ì•¼ í•  ë¬¸ì„œ ìˆœì„œ

```
1ì°¨: ì´ ë¬¸ì„œ (SONNET_DEV_GUIDE.md)        â† ì§€ê¸ˆ ì—¬ê¸°
2ì°¨: DATABASE_SCHEMA.md                    â† DB ê°œë°œí•  ë•Œ
3ì°¨: CONTROL_SYSTEM.md                     â† API ê°œë°œí•  ë•Œ
4ì°¨: TECH_STACK.md                         â† Frontend ê¸°ë³¸
5ì°¨: ANIMATION_SPEC.md                     â† ì‹œê°í™” êµ¬í˜„
```

### ğŸ¯ ê°œë°œ ì² í•™

**í•œ ë²ˆì— í•œ ë‹¨ê³„ì”©!**
- ê° Phaseë¥¼ ì™„ë£Œí•œ í›„ ë‹¤ìŒìœ¼ë¡œ
- ì½”ë“œë¥¼ ë³µì‚¬-ë¶™ì—¬ë„£ê¸°
- í…ŒìŠ¤íŠ¸ í›„ ì»¤ë°‹

---

## Phase 1: í”„ë¡œì íŠ¸ ìƒì„±

### ğŸ¯ ëª©í‘œ
ë¹ˆ í”„ë¡œì íŠ¸ë¥¼ ìƒì„±í•˜ê³  Docker Composeë¡œ ì‹¤í–‰

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
- [ ] Docker Compose ì„¤ì •
- [ ] Backend ê¸°ë³¸ êµ¬ì¡°
- [ ] Frontend ê¸°ë³¸ êµ¬ì¡°
- [ ] ì ‘ì† í™•ì¸

### ğŸš€ ì‹¤í–‰

#### Step 1.1: ìë™ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ì¶”ì²œ)

```bash
# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ í™•ì¸
ls ~/Dev/aegis/v2/docs/dev3/ai-visualizer/create_project.sh

# ì‹¤í–‰
bash ~/Dev/aegis/v2/docs/dev3/ai-visualizer/create_project.sh

# ì™„ë£Œ ë©”ì‹œì§€ í™•ì¸
# âœ… í”„ë¡œì íŠ¸ ìƒì„± ì™„ë£Œ!
# ğŸ“ í”„ë¡œì íŠ¸ ìœ„ì¹˜: ~/Dev/aegis-visualizer
```

#### Step 1.2: í”„ë¡œì íŠ¸ë¡œ ì´ë™

```bash
cd ~/Dev/aegis-visualizer

# êµ¬ì¡° í™•ì¸
tree -L 2 -I 'node_modules|venv|__pycache__'

# ì˜ˆìƒ ì¶œë ¥:
# .
# â”œâ”€â”€ backend/
# â”‚   â”œâ”€â”€ app/
# â”‚   â”œâ”€â”€ requirements.txt
# â”‚   â””â”€â”€ Dockerfile
# â”œâ”€â”€ frontend/
# â”‚   â”œâ”€â”€ src/
# â”‚   â”œâ”€â”€ package.json
# â”‚   â””â”€â”€ Dockerfile
# â””â”€â”€ docker-compose.yml
```

#### Step 1.3: Docker Compose ì‹¤í–‰

```bash
# ë¹Œë“œ ë° ì‹¤í–‰
docker-compose up -d

# ë¡œê·¸ í™•ì¸ (10ì´ˆ ì •ë„ ëŒ€ê¸°)
docker-compose logs -f

# Ctrl+Cë¡œ ë¡œê·¸ ì¤‘ë‹¨

# ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker-compose ps

# ì˜ˆìƒ ì¶œë ¥:
# NAME                  STATUS
# visualizer_db         Up (healthy)
# visualizer_backend    Up
# visualizer_frontend   Up
```

#### Step 1.4: ì ‘ì† í…ŒìŠ¤íŠ¸

```bash
# Backend API í…ŒìŠ¤íŠ¸
curl http://localhost:8001/health

# ì˜ˆìƒ ì¶œë ¥:
# {"status":"healthy"}

# Frontend í…ŒìŠ¤íŠ¸ (ë¸Œë¼ìš°ì €)
# http://localhost:5174
# "ğŸš€ AEGIS Visualizer" ë³´ì´ë©´ ì„±ê³µ!

# API ë¬¸ì„œ
# http://localhost:8001/docs
```

### âœ… Phase 1 ì™„ë£Œ ì¡°ê±´

- [ ] `docker-compose ps`ì—ì„œ ëª¨ë“  ì»¨í…Œì´ë„ˆ Up
- [ ] http://localhost:8001/health â†’ `{"status":"healthy"}`
- [ ] http://localhost:5174 â†’ Frontend í™”ë©´ ë³´ì„
- [ ] http://localhost:8001/docs â†’ Swagger UI ë³´ì„

### ğŸ› ë¬¸ì œ ë°œìƒ ì‹œ

```bash
# ì»¨í…Œì´ë„ˆ ì¤‘ì§€
docker-compose down

# ë³¼ë¥¨ í¬í•¨ ì™„ì „ ì‚­ì œ
docker-compose down -v

# ì¬ì‹œì‘
docker-compose up -d --build
```

---

## Phase 2: Database ê°œë°œ

### ğŸ¯ ëª©í‘œ
PostgreSQLì— 8ê°œ í…Œì´ë¸” ìƒì„± ë° ì´ˆê¸° ë°ì´í„° ì…ë ¥

### ğŸ“š ì°¸ì¡° ë¬¸ì„œ
**DATABASE_SCHEMA.md** (28KB)ë¥¼ ì½ì–´ì£¼ì„¸ìš”.

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Alembic ì„¤ì •
- [ ] ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ ìƒì„±
- [ ] 8ê°œ í…Œì´ë¸” ìƒì„±
- [ ] ì¸ë±ìŠ¤ ì¶”ê°€
- [ ] ì´ˆê¸° ë°ì´í„° ì…ë ¥
- [ ] ê²€ì¦

### ğŸš€ ì‹¤í–‰

#### Step 2.1: Alembic ì´ˆê¸°í™”

```bash
cd ~/Dev/aegis-visualizer/backend

# ê°€ìƒí™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt

# Alembic ì´ˆê¸°í™” (ì´ë¯¸ ë””ë ‰í† ë¦¬ ìˆìœ¼ë©´ ìŠ¤í‚µ)
alembic init alembic
```

#### Step 2.2: Alembic ì„¤ì • ìˆ˜ì •

**íŒŒì¼: `alembic.ini`**

```ini
# ê¸°ì¡´:
# sqlalchemy.url = driver://user:pass@localhost/dbname

# ë³€ê²½:
sqlalchemy.url = postgresql://visualizer_admin:visualizer2024@localhost:5433/visualizer
```

**íŒŒì¼: `alembic/env.py`**

```python
# ë§¨ ìœ„ì— ì¶”ê°€
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

from app.database import Base
from app.models.visualizer import *  # ëª¨ë“  ëª¨ë¸ ì„í¬íŠ¸

# target_metadata ìˆ˜ì •
target_metadata = Base.metadata
```

#### Step 2.3: ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„±

```bash
# ë§ˆì´ê·¸ë ˆì´ì…˜ ìƒì„±
alembic revision --autogenerate -m "create_visualizer_tables"

# ìƒì„±ëœ íŒŒì¼ í™•ì¸
ls alembic/versions/

# ì˜ˆ: abc123_create_visualizer_tables.py
```

#### Step 2.4: ë§ˆì´ê·¸ë ˆì´ì…˜ íŒŒì¼ ìˆ˜ì •

**íŒŒì¼: `alembic/versions/xxx_create_visualizer_tables.py`**

**DATABASE_SCHEMA.mdì˜ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë³µì‚¬-ë¶™ì—¬ë„£ê¸°:**

```python
"""create visualizer tables

Revision ID: xxx
Revises:
Create Date: 2025-12-08

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = 'xxx'  # ìë™ ìƒì„±ëœ ê°’ ìœ ì§€
down_revision = None
branch_labels = None
depends_on = None


def upgrade():
    # 1. analysis_batches
    op.create_table(
        'analysis_batches',
        sa.Column('id', postgresql.UUID(as_uuid=True), primary_key=True, server_default=sa.text('gen_random_uuid()')),
        sa.Column('started_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
        sa.Column('completed_at', sa.TIMESTAMP(), nullable=True),
        sa.Column('status', sa.VARCHAR(20), nullable=False, server_default='RUNNING'),
        sa.Column('trigger_type', sa.VARCHAR(20), nullable=False),
        sa.Column('error_message', sa.TEXT(), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=True),
        sa.Column('created_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
        sa.Column('updated_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
    )
    op.create_index('idx_batches_status', 'analysis_batches', ['status'])
    op.create_index('idx_batches_started_at', 'analysis_batches', ['started_at'], postgresql_ops={'started_at': 'DESC'})

    # 2. signal_sources
    op.create_table(
        'signal_sources',
        sa.Column('id', sa.INTEGER(), primary_key=True, autoincrement=True),
        sa.Column('source_code', sa.VARCHAR(50), unique=True, nullable=False),
        sa.Column('source_name', sa.VARCHAR(100), nullable=False),
        sa.Column('category', sa.VARCHAR(50), nullable=False),
        sa.Column('region', sa.VARCHAR(50), nullable=False),
        sa.Column('icon', sa.VARCHAR(10), nullable=True),
        sa.Column('position_x', sa.FLOAT(), nullable=True),
        sa.Column('position_y', sa.FLOAT(), nullable=True),
        sa.Column('is_active', sa.BOOLEAN(), nullable=False, server_default='TRUE'),
        sa.Column('created_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
        sa.Column('updated_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
    )
    op.create_index('idx_sources_category', 'signal_sources', ['category'])
    op.create_index('idx_sources_region', 'signal_sources', ['region'])

    # 3. signal_logs
    op.create_table(
        'signal_logs',
        sa.Column('id', sa.INTEGER(), primary_key=True, autoincrement=True),
        sa.Column('batch_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('analysis_batches.id', ondelete='CASCADE'), nullable=False),
        sa.Column('source_code', sa.VARCHAR(50), sa.ForeignKey('signal_sources.source_code'), nullable=False),
        sa.Column('signal_type', sa.VARCHAR(20), nullable=False),
        sa.Column('title', sa.VARCHAR(500), nullable=True),
        sa.Column('content', sa.TEXT(), nullable=True),
        sa.Column('sentiment', sa.VARCHAR(20), nullable=True),
        sa.Column('sentiment_score', sa.FLOAT(), nullable=True),
        sa.Column('impact_level', sa.VARCHAR(20), nullable=True),
        sa.Column('raw_value', postgresql.JSONB(), nullable=True),
        sa.Column('fetched_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
    )
    op.create_index('idx_signals_batch', 'signal_logs', ['batch_id'])
    op.create_index('idx_signals_source', 'signal_logs', ['source_code'])

    # 4. analysis_steps
    op.create_table(
        'analysis_steps',
        sa.Column('id', sa.INTEGER(), primary_key=True, autoincrement=True),
        sa.Column('batch_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('analysis_batches.id', ondelete='CASCADE'), nullable=False),
        sa.Column('step_name', sa.VARCHAR(50), nullable=False),
        sa.Column('model_used', sa.VARCHAR(50), nullable=True),
        sa.Column('input_count', sa.INTEGER(), nullable=True),
        sa.Column('output_count', sa.INTEGER(), nullable=True),
        sa.Column('processing_time_ms', sa.INTEGER(), nullable=True),
        sa.Column('started_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
        sa.Column('completed_at', sa.TIMESTAMP(), nullable=True),
        sa.Column('error_message', sa.TEXT(), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=True),
    )
    op.create_index('idx_steps_batch', 'analysis_steps', ['batch_id'])
    op.create_index('idx_steps_name', 'analysis_steps', ['step_name'])

    # 5. analysis_stocks
    op.create_table(
        'analysis_stocks',
        sa.Column('id', sa.INTEGER(), primary_key=True, autoincrement=True),
        sa.Column('batch_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('analysis_batches.id', ondelete='CASCADE'), nullable=False),
        sa.Column('step_name', sa.VARCHAR(50), nullable=False),
        sa.Column('stock_code', sa.VARCHAR(20), nullable=False),
        sa.Column('stock_name', sa.VARCHAR(100), nullable=True),
        sa.Column('status', sa.VARCHAR(20), nullable=False),
        sa.Column('score', sa.NUMERIC(10, 4), nullable=True),
        sa.Column('filter_reason', sa.TEXT(), nullable=True),
        sa.Column('position_x', sa.FLOAT(), nullable=True),
        sa.Column('position_y', sa.FLOAT(), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=True),
        sa.Column('created_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
    )
    op.create_index('idx_stocks_batch', 'analysis_stocks', ['batch_id'])
    op.create_index('idx_stocks_batch_step', 'analysis_stocks', ['batch_id', 'step_name'])

    # 6. signal_stock_impacts
    op.create_table(
        'signal_stock_impacts',
        sa.Column('id', sa.INTEGER(), primary_key=True, autoincrement=True),
        sa.Column('batch_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('analysis_batches.id', ondelete='CASCADE'), nullable=False),
        sa.Column('signal_id', sa.INTEGER(), sa.ForeignKey('signal_logs.id', ondelete='CASCADE'), nullable=False),
        sa.Column('stock_code', sa.VARCHAR(20), nullable=False),
        sa.Column('impact_type', sa.VARCHAR(20), nullable=False),
        sa.Column('impact_score', sa.FLOAT(), nullable=True),
        sa.Column('reasoning', sa.TEXT(), nullable=True),
        sa.Column('created_at', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
    )
    op.create_index('idx_impacts_batch', 'signal_stock_impacts', ['batch_id'])
    op.create_index('idx_impacts_signal', 'signal_stock_impacts', ['signal_id'])

    # 7. control_events
    op.create_table(
        'control_events',
        sa.Column('id', sa.INTEGER(), primary_key=True, autoincrement=True),
        sa.Column('event_type', sa.VARCHAR(50), nullable=False),
        sa.Column('batch_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('analysis_batches.id', ondelete='SET NULL'), nullable=True),
        sa.Column('user_id', sa.VARCHAR(100), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=True),
        sa.Column('timestamp', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
    )
    op.create_index('idx_events_type', 'control_events', ['event_type'])
    op.create_index('idx_events_timestamp', 'control_events', ['timestamp'], postgresql_ops={'timestamp': 'DESC'})

    # 8. performance_metrics
    op.create_table(
        'performance_metrics',
        sa.Column('id', sa.INTEGER(), primary_key=True, autoincrement=True),
        sa.Column('metric_type', sa.VARCHAR(50), nullable=False),
        sa.Column('metric_name', sa.VARCHAR(100), nullable=False),
        sa.Column('value', sa.NUMERIC(20, 4), nullable=False),
        sa.Column('unit', sa.VARCHAR(20), nullable=True),
        sa.Column('batch_id', postgresql.UUID(as_uuid=True), sa.ForeignKey('analysis_batches.id', ondelete='SET NULL'), nullable=True),
        sa.Column('metadata', postgresql.JSONB(), nullable=True),
        sa.Column('timestamp', sa.TIMESTAMP(), nullable=False, server_default=sa.text('NOW()')),
    )
    op.create_index('idx_metrics_type', 'performance_metrics', ['metric_type'])
    op.create_index('idx_metrics_timestamp', 'performance_metrics', ['timestamp'], postgresql_ops={'timestamp': 'DESC'})


def downgrade():
    op.drop_table('performance_metrics')
    op.drop_table('control_events')
    op.drop_table('signal_stock_impacts')
    op.drop_table('analysis_stocks')
    op.drop_table('analysis_steps')
    op.drop_table('signal_logs')
    op.drop_table('signal_sources')
    op.drop_table('analysis_batches')
```

#### Step 2.5: ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰

```bash
# ë§ˆì´ê·¸ë ˆì´ì…˜ ì ìš©
alembic upgrade head

# ì„±ê³µ ë©”ì‹œì§€ í™•ì¸:
# INFO [alembic.runtime.migration] Running upgrade  -> xxx, create visualizer tables
```

#### Step 2.6: í…Œì´ë¸” í™•ì¸

```bash
# psql ì ‘ì†
psql -h localhost -p 5433 -U visualizer_admin -d visualizer

# í…Œì´ë¸” ëª©ë¡ í™•ì¸
\dt

# ì˜ˆìƒ ì¶œë ¥:
#                    List of relations
#  Schema |          Name           | Type  |      Owner
# --------+-------------------------+-------+------------------
#  public | analysis_batches        | table | visualizer_admin
#  public | analysis_steps          | table | visualizer_admin
#  public | analysis_stocks         | table | visualizer_admin
#  public | control_events          | table | visualizer_admin
#  public | performance_metrics     | table | visualizer_admin
#  public | signal_logs             | table | visualizer_admin
#  public | signal_sources          | table | visualizer_admin
#  public | signal_stock_impacts    | table | visualizer_admin

# íŠ¹ì • í…Œì´ë¸” êµ¬ì¡° í™•ì¸
\d analysis_batches

# ì¢…ë£Œ
\q
```

#### Step 2.7: ì´ˆê¸° ë°ì´í„° ì…ë ¥

**íŒŒì¼: `backend/scripts/seed_data.py` ìƒì„±**

```python
# backend/scripts/seed_data.py
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

import asyncio
from app.database import AsyncSessionLocal
from app.models.visualizer import SignalSource


async def seed_signal_sources():
    """ì‹ í˜¸ ì†ŒìŠ¤ ì´ˆê¸° ë°ì´í„°"""
    sources = [
        {
            'source_code': 'US_FED',
            'source_name': 'ë¯¸êµ­ ì—°ì¤€ ê¸ˆë¦¬',
            'category': 'MACRO',
            'region': 'US',
            'icon': 'ğŸ‡ºğŸ‡¸',
            'position_x': 0.2,
            'position_y': 0.3,
        },
        {
            'source_code': 'US_SP500',
            'source_name': 'S&P 500',
            'category': 'INDEX',
            'region': 'US',
            'icon': 'ğŸ“ˆ',
            'position_x': 0.25,
            'position_y': 0.35,
        },
        {
            'source_code': 'EU_ECB',
            'source_name': 'ìœ ëŸ½ ì¤‘ì•™ì€í–‰',
            'category': 'MACRO',
            'region': 'EU',
            'icon': 'ğŸ‡ªğŸ‡º',
            'position_x': 0.4,
            'position_y': 0.2,
        },
        {
            'source_code': 'GOLD',
            'source_name': 'ê¸ˆ ê°€ê²©',
            'category': 'COMMODITY',
            'region': 'GLOBAL',
            'icon': 'ğŸ¥‡',
            'position_x': 0.5,
            'position_y': 0.5,
        },
        {
            'source_code': 'WTI',
            'source_name': 'WTI ìœ ê°€',
            'category': 'COMMODITY',
            'region': 'GLOBAL',
            'icon': 'ğŸ›¢ï¸',
            'position_x': 0.6,
            'position_y': 0.5,
        },
        {
            'source_code': 'COPPER',
            'source_name': 'êµ¬ë¦¬ ê°€ê²©',
            'category': 'COMMODITY',
            'region': 'GLOBAL',
            'icon': 'ğŸ”¶',
            'position_x': 0.65,
            'position_y': 0.55,
        },
        {
            'source_code': 'JP_NIKKEI',
            'source_name': 'ë‹ˆì¼€ì´ 225',
            'category': 'INDEX',
            'region': 'ASIA',
            'icon': 'ğŸ‡¯ğŸ‡µ',
            'position_x': 0.7,
            'position_y': 0.3,
        },
        {
            'source_code': 'CN_SSE',
            'source_name': 'ìƒí•´ì¢…í•©',
            'category': 'INDEX',
            'region': 'ASIA',
            'icon': 'ğŸ‡¨ğŸ‡³',
            'position_x': 0.75,
            'position_y': 0.4,
        },
    ]

    async with AsyncSessionLocal() as session:
        for data in sources:
            source = SignalSource(**data)
            session.add(source)

        await session.commit()
        print(f"âœ… {len(sources)}ê°œ ì‹ í˜¸ ì†ŒìŠ¤ ìƒì„± ì™„ë£Œ")


if __name__ == '__main__':
    asyncio.run(seed_signal_sources())
```

**ì‹¤í–‰:**

```bash
cd ~/Dev/aegis-visualizer/backend

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python scripts/seed_data.py

# í™•ì¸
psql -h localhost -p 5433 -U visualizer_admin -d visualizer -c "SELECT source_code, source_name, icon FROM signal_sources;"

# ì˜ˆìƒ ì¶œë ¥:
#  source_code  |  source_name    | icon
# --------------+-----------------+------
#  US_FED       | ë¯¸êµ­ ì—°ì¤€ ê¸ˆë¦¬  | ğŸ‡ºğŸ‡¸
#  US_SP500     | S&P 500         | ğŸ“ˆ
#  ...
```

### âœ… Phase 2 ì™„ë£Œ ì¡°ê±´

- [ ] `alembic upgrade head` ì„±ê³µ
- [ ] `\dt` ëª…ë ¹ìœ¼ë¡œ 8ê°œ í…Œì´ë¸” í™•ì¸
- [ ] signal_sourcesì— 8ê°œ ë°ì´í„° í™•ì¸
- [ ] ëª¨ë“  ì¸ë±ìŠ¤ ìƒì„± í™•ì¸

---

## Phase 3: Backend API ê°œë°œ

### ğŸ¯ ëª©í‘œ
FastAPIë¡œ REST + WebSocket API êµ¬í˜„

### ğŸ“š ì°¸ì¡° ë¬¸ì„œ
**CONTROL_SYSTEM.md** (28KB)ë¥¼ ì½ì–´ì£¼ì„¸ìš”.

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ëª¨ë¸ í´ë˜ìŠ¤ ì™„ì„±
- [ ] API ë¼ìš°í„° ìƒì„±
- [ ] WebSocket êµ¬í˜„
- [ ] í…ŒìŠ¤íŠ¸

### ğŸš€ ì‹¤í–‰

#### Step 3.1: ëª¨ë¸ ì™„ì„±

**íŒŒì¼: `backend/app/models/visualizer.py` ìˆ˜ì •**

```python
# backend/app/models/visualizer.py
from sqlalchemy import Column, Integer, String, Float, Boolean, Text, TIMESTAMP, ForeignKey
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.orm import relationship
from sqlalchemy.sql import func
import uuid

from app.database import Base


class AnalysisBatch(Base):
    __tablename__ = 'analysis_batches'

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
    started_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    completed_at = Column(TIMESTAMP)
    status = Column(String(20), nullable=False, default='RUNNING')
    trigger_type = Column(String(20), nullable=False)
    error_message = Column(Text)
    metadata = Column(JSONB)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=False, server_default=func.now(), onupdate=func.now())

    # ê´€ê³„
    steps = relationship('AnalysisStep', back_populates='batch', cascade='all, delete-orphan')
    signals = relationship('SignalLog', back_populates='batch', cascade='all, delete-orphan')
    stocks = relationship('AnalysisStock', back_populates='batch', cascade='all, delete-orphan')


class SignalSource(Base):
    __tablename__ = 'signal_sources'

    id = Column(Integer, primary_key=True, autoincrement=True)
    source_code = Column(String(50), unique=True, nullable=False)
    source_name = Column(String(100), nullable=False)
    category = Column(String(50), nullable=False)
    region = Column(String(50), nullable=False)
    icon = Column(String(10))
    position_x = Column(Float)
    position_y = Column(Float)
    is_active = Column(Boolean, default=True)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    updated_at = Column(TIMESTAMP, nullable=False, server_default=func.now(), onupdate=func.now())

    signals = relationship('SignalLog', back_populates='source')


class SignalLog(Base):
    __tablename__ = 'signal_logs'

    id = Column(Integer, primary_key=True, autoincrement=True)
    batch_id = Column(UUID(as_uuid=True), ForeignKey('analysis_batches.id', ondelete='CASCADE'), nullable=False)
    source_code = Column(String(50), ForeignKey('signal_sources.source_code'), nullable=False)
    signal_type = Column(String(20), nullable=False)
    title = Column(String(500))
    content = Column(Text)
    sentiment = Column(String(20))
    sentiment_score = Column(Float)
    impact_level = Column(String(20))
    raw_value = Column(JSONB)
    fetched_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    batch = relationship('AnalysisBatch', back_populates='signals')
    source = relationship('SignalSource', back_populates='signals')


class AnalysisStep(Base):
    __tablename__ = 'analysis_steps'

    id = Column(Integer, primary_key=True, autoincrement=True)
    batch_id = Column(UUID(as_uuid=True), ForeignKey('analysis_batches.id', ondelete='CASCADE'), nullable=False)
    step_name = Column(String(50), nullable=False)
    model_used = Column(String(50))
    input_count = Column(Integer)
    output_count = Column(Integer)
    processing_time_ms = Column(Integer)
    started_at = Column(TIMESTAMP, nullable=False, server_default=func.now())
    completed_at = Column(TIMESTAMP)
    error_message = Column(Text)
    metadata = Column(JSONB)

    batch = relationship('AnalysisBatch', back_populates='steps')


class AnalysisStock(Base):
    __tablename__ = 'analysis_stocks'

    id = Column(Integer, primary_key=True, autoincrement=True)
    batch_id = Column(UUID(as_uuid=True), ForeignKey('analysis_batches.id', ondelete='CASCADE'), nullable=False)
    step_name = Column(String(50), nullable=False)
    stock_code = Column(String(20), nullable=False)
    stock_name = Column(String(100))
    status = Column(String(20), nullable=False)
    score = Column(Float)
    filter_reason = Column(Text)
    position_x = Column(Float)
    position_y = Column(Float)
    metadata = Column(JSONB)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())

    batch = relationship('AnalysisBatch', back_populates='stocks')


class SignalStockImpact(Base):
    __tablename__ = 'signal_stock_impacts'

    id = Column(Integer, primary_key=True, autoincrement=True)
    batch_id = Column(UUID(as_uuid=True), ForeignKey('analysis_batches.id', ondelete='CASCADE'), nullable=False)
    signal_id = Column(Integer, ForeignKey('signal_logs.id', ondelete='CASCADE'), nullable=False)
    stock_code = Column(String(20), nullable=False)
    impact_type = Column(String(20), nullable=False)
    impact_score = Column(Float)
    reasoning = Column(Text)
    created_at = Column(TIMESTAMP, nullable=False, server_default=func.now())


class ControlEvent(Base):
    __tablename__ = 'control_events'

    id = Column(Integer, primary_key=True, autoincrement=True)
    event_type = Column(String(50), nullable=False)
    batch_id = Column(UUID(as_uuid=True), ForeignKey('analysis_batches.id', ondelete='SET NULL'))
    user_id = Column(String(100))
    metadata = Column(JSONB)
    timestamp = Column(TIMESTAMP, nullable=False, server_default=func.now())


class PerformanceMetric(Base):
    __tablename__ = 'performance_metrics'

    id = Column(Integer, primary_key=True, autoincrement=True)
    metric_type = Column(String(50), nullable=False)
    metric_name = Column(String(100), nullable=False)
    value = Column(Float, nullable=False)
    unit = Column(String(20))
    batch_id = Column(UUID(as_uuid=True), ForeignKey('analysis_batches.id', ondelete='SET NULL'))
    metadata = Column(JSONB)
    timestamp = Column(TIMESTAMP, nullable=False, server_default=func.now())
```

#### Step 3.2: API ë¼ìš°í„° ìƒì„±

**CONTROL_SYSTEM.mdì˜ ì½”ë“œë¥¼ ë³µì‚¬-ë¶™ì—¬ë„£ê¸°:**

**íŒŒì¼: `backend/app/api/control.py`**

```python
# backend/app/api/control.py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from typing import List, Optional

from app.database import get_db
from app.models.visualizer import AnalysisBatch

router = APIRouter()


@router.get("/batches")
async def list_batches(
    status: Optional[str] = None,
    limit: int = 20,
    db: AsyncSession = Depends(get_db),
):
    """ì‹¤í–‰ ì¤‘ì¸ ë°°ì¹˜ ëª©ë¡"""
    query = select(AnalysisBatch).order_by(AnalysisBatch.started_at.desc())

    if status:
        query = query.where(AnalysisBatch.status == status)

    query = query.limit(limit)

    result = await db.execute(query)
    batches = result.scalars().all()

    return {
        "batches": [
            {
                "id": str(batch.id),
                "status": batch.status,
                "trigger_type": batch.trigger_type,
                "started_at": batch.started_at.isoformat() if batch.started_at else None,
                "completed_at": batch.completed_at.isoformat() if batch.completed_at else None,
            }
            for batch in batches
        ]
    }


@router.post("/batch/start")
async def start_batch(
    trigger_type: str = "MANUAL",
    db: AsyncSession = Depends(get_db),
):
    """ìƒˆ ë°°ì¹˜ ì‹œì‘"""
    batch = AnalysisBatch(
        trigger_type=trigger_type,
        status='RUNNING',
    )
    db.add(batch)
    await db.commit()
    await db.refresh(batch)

    return {
        "batch_id": str(batch.id),
        "status": "RUNNING",
        "message": "Batch started successfully",
    }


@router.post("/batch/{batch_id}/stop")
async def stop_batch(
    batch_id: str,
    db: AsyncSession = Depends(get_db),
):
    """ë°°ì¹˜ ì¤‘ì§€"""
    result = await db.execute(
        select(AnalysisBatch).where(AnalysisBatch.id == batch_id)
    )
    batch = result.scalar_one_or_none()

    if not batch:
        raise HTTPException(status_code=404, detail="Batch not found")

    if batch.status != "RUNNING":
        raise HTTPException(status_code=400, detail="Batch is not running")

    batch.status = "PAUSED"
    await db.commit()

    return {
        "batch_id": str(batch.id),
        "status": "PAUSED",
        "message": "Batch stopped successfully",
    }


@router.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy"}
```

#### Step 3.3: main.pyì— ë¼ìš°í„° ë“±ë¡

**íŒŒì¼: `backend/app/main.py` ìˆ˜ì •**

```python
# backend/app/main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager

from app.database import engine
from app.api import control  # ì¶”ê°€

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    yield
    # Shutdown
    await engine.dispose()


app = FastAPI(
    title="AEGIS Visualizer API",
    description="AI Reasoning Visualizer Backend",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:5174"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Routes
app.include_router(control.router, prefix="/api/control", tags=["control"])


@app.get("/")
def read_root():
    return {
        "message": "AEGIS Visualizer API",
        "version": "1.0.0",
        "docs": "/docs",
    }


@app.get("/health")
def health_check():
    return {"status": "healthy"}
```

#### Step 3.4: í…ŒìŠ¤íŠ¸

```bash
# Backend ì¬ì‹œì‘ (Docker)
docker-compose restart backend

# ë˜ëŠ” ë¡œì»¬
cd ~/Dev/aegis-visualizer/backend
source venv/bin/activate
uvicorn app.main:app --reload --port 8001
```

**API í…ŒìŠ¤íŠ¸:**

```bash
# 1. í—¬ìŠ¤ ì²´í¬
curl http://localhost:8001/api/control/health

# 2. ë°°ì¹˜ ëª©ë¡ (ë¹ˆ ë°°ì—´)
curl http://localhost:8001/api/control/batches

# 3. ë°°ì¹˜ ì‹œì‘
curl -X POST http://localhost:8001/api/control/batch/start?trigger_type=MANUAL

# ì‘ë‹µ ì˜ˆì‹œ:
# {
#   "batch_id": "550e8400-e29b-41d4-a716-446655440000",
#   "status": "RUNNING",
#   "message": "Batch started successfully"
# }

# 4. ë°°ì¹˜ ëª©ë¡ ì¬ì¡°íšŒ (1ê°œ ìˆìŒ)
curl http://localhost:8001/api/control/batches

# 5. Swagger UI í™•ì¸
# http://localhost:8001/docs
```

### âœ… Phase 3 ì™„ë£Œ ì¡°ê±´

- [ ] ëª¨ë“  API ì—”ë“œí¬ì¸íŠ¸ ë™ì‘
- [ ] POST /batch/start â†’ ë°°ì¹˜ ìƒì„±ë¨
- [ ] GET /batches â†’ ëª©ë¡ ì¡°íšŒë¨
- [ ] Swagger UIì—ì„œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

---

## Phase 4: Frontend ê¸°ë³¸ ê°œë°œ

### ğŸ¯ ëª©í‘œ
React ê¸°ë³¸ êµ¬ì¡° + API ì—°ë™

### ğŸ“š ì°¸ì¡° ë¬¸ì„œ
**TECH_STACK.md** (20KB)ë¥¼ ì½ì–´ì£¼ì„¸ìš”.

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] íŒ¨í‚¤ì§€ ì„¤ì¹˜
- [ ] API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
- [ ] React Query ì„¤ì •
- [ ] ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸
- [ ] API ì—°ë™ í…ŒìŠ¤íŠ¸

### ğŸš€ ì‹¤í–‰

#### Step 4.1: íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
cd ~/Dev/aegis-visualizer/frontend

# íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì´ë¯¸ package.jsonì— ìˆìŒ)
pnpm install

# ì¶”ê°€ íŒ¨í‚¤ì§€ (í•„ìš” ì‹œ)
pnpm add @tanstack/react-query axios
pnpm add @radix-ui/react-dialog @radix-ui/react-tabs
pnpm add tailwindcss postcss autoprefixer
pnpm add lucide-react

# TailwindCSS ì´ˆê¸°í™”
pnpm dlx tailwindcss init -p
```

#### Step 4.2: API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •

**íŒŒì¼: `frontend/src/lib/api.ts` ìƒì„±**

```typescript
// frontend/src/lib/api.ts
import axios from 'axios';

const API_URL = import.meta.env.VITE_API_URL || 'http://localhost:8001';

export const api = axios.create({
  baseURL: API_URL,
  timeout: 30000,
  headers: {
    'Content-Type': 'application/json',
  },
});

// íƒ€ì… ì •ì˜
export interface AnalysisBatch {
  id: string;
  status: 'RUNNING' | 'COMPLETED' | 'FAILED' | 'PAUSED';
  trigger_type: string;
  started_at: string;
  completed_at?: string;
}

// API í•¨ìˆ˜ë“¤
export const batchAPI = {
  list: async () => {
    const response = await api.get('/api/control/batches');
    return response.data;
  },

  start: async (trigger_type: string = 'MANUAL') => {
    const response = await api.post(`/api/control/batch/start?trigger_type=${trigger_type}`);
    return response.data;
  },

  stop: async (batchId: string) => {
    const response = await api.post(`/api/control/batch/${batchId}/stop`);
    return response.data;
  },
};
```

#### Step 4.3: React Query ì„¤ì •

**íŒŒì¼: `frontend/src/App.tsx` ìˆ˜ì •**

```typescript
// frontend/src/App.tsx
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import { useState } from 'react';
import { BatchList } from './components/BatchList';

const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      staleTime: 1000 * 60 * 5,
      gcTime: 1000 * 60 * 30,
      refetchOnWindowFocus: false,
    },
  },
});

function App() {
  return (
    <QueryClientProvider client={queryClient}>
      <div className="w-full min-h-screen bg-gray-900 text-white p-8">
        <h1 className="text-4xl font-bold mb-8">ğŸš€ AEGIS Visualizer</h1>
        <BatchList />
      </div>
    </QueryClientProvider>
  );
}

export default App;
```

#### Step 4.4: ë°°ì¹˜ ëª©ë¡ ì»´í¬ë„ŒíŠ¸

**íŒŒì¼: `frontend/src/components/BatchList.tsx` ìƒì„±**

```typescript
// frontend/src/components/BatchList.tsx
import { useQuery, useMutation, useQueryClient } from '@tanstack/react-query';
import { batchAPI, type AnalysisBatch } from '../lib/api';

export function BatchList() {
  const queryClient = useQueryClient();

  // ë°°ì¹˜ ëª©ë¡ ì¡°íšŒ
  const { data, isLoading, error } = useQuery({
    queryKey: ['batches'],
    queryFn: batchAPI.list,
    refetchInterval: 2000, // 2ì´ˆë§ˆë‹¤ ìë™ ê°±ì‹ 
  });

  // ë°°ì¹˜ ì‹œì‘
  const startMutation = useMutation({
    mutationFn: batchAPI.start,
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['batches'] });
    },
  });

  // ë°°ì¹˜ ì¤‘ì§€
  const stopMutation = useMutation({
    mutationFn: batchAPI.stop,
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: ['batches'] });
    },
  });

  if (isLoading) {
    return <div>Loading...</div>;
  }

  if (error) {
    return <div className="text-red-500">Error: {(error as Error).message}</div>;
  }

  const batches: AnalysisBatch[] = data?.batches || [];

  return (
    <div className="space-y-4">
      <div className="flex justify-between items-center">
        <h2 className="text-2xl font-bold">ë°°ì¹˜ ëª©ë¡</h2>
        <button
          onClick={() => startMutation.mutate()}
          disabled={startMutation.isPending}
          className="px-4 py-2 bg-blue-600 hover:bg-blue-700 rounded disabled:opacity-50"
        >
          {startMutation.isPending ? 'ì‹œì‘ ì¤‘...' : 'ìƒˆ ë°°ì¹˜ ì‹œì‘'}
        </button>
      </div>

      {batches.length === 0 ? (
        <div className="text-gray-400">ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆ ë°°ì¹˜ë¥¼ ì‹œì‘í•˜ì„¸ìš”.</div>
      ) : (
        <div className="grid gap-4">
          {batches.map((batch) => (
            <div
              key={batch.id}
              className="p-4 bg-gray-800 rounded-lg border border-gray-700"
            >
              <div className="flex justify-between items-start">
                <div>
                  <div className="text-sm text-gray-400">Batch ID</div>
                  <div className="font-mono text-sm">{batch.id}</div>
                </div>
                <div>
                  <span
                    className={`px-3 py-1 rounded text-sm ${
                      batch.status === 'RUNNING'
                        ? 'bg-yellow-600'
                        : batch.status === 'COMPLETED'
                        ? 'bg-green-600'
                        : batch.status === 'FAILED'
                        ? 'bg-red-600'
                        : 'bg-gray-600'
                    }`}
                  >
                    {batch.status}
                  </span>
                </div>
              </div>

              <div className="mt-4 grid grid-cols-2 gap-4 text-sm">
                <div>
                  <div className="text-gray-400">Trigger Type</div>
                  <div>{batch.trigger_type}</div>
                </div>
                <div>
                  <div className="text-gray-400">Started At</div>
                  <div>{new Date(batch.started_at).toLocaleString()}</div>
                </div>
              </div>

              {batch.status === 'RUNNING' && (
                <div className="mt-4">
                  <button
                    onClick={() => stopMutation.mutate(batch.id)}
                    disabled={stopMutation.isPending}
                    className="px-3 py-1 bg-red-600 hover:bg-red-700 rounded text-sm disabled:opacity-50"
                  >
                    {stopMutation.isPending ? 'ì¤‘ì§€ ì¤‘...' : 'ì¤‘ì§€'}
                  </button>
                </div>
              )}
            </div>
          ))}
        </div>
      )}
    </div>
  );
}
```

#### Step 4.5: í…ŒìŠ¤íŠ¸

```bash
# Frontend ì‹¤í–‰
cd ~/Dev/aegis-visualizer/frontend
pnpm dev

# ë¸Œë¼ìš°ì € ì—´ê¸°
# http://localhost:5174
```

**í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:**
1. âœ… "ìƒˆ ë°°ì¹˜ ì‹œì‘" ë²„íŠ¼ í´ë¦­
2. âœ… ë°°ì¹˜ ì¹´ë“œ ìƒì„± í™•ì¸
3. âœ… Statusê°€ "RUNNING" í™•ì¸
4. âœ… "ì¤‘ì§€" ë²„íŠ¼ í´ë¦­
5. âœ… Statusê°€ "PAUSED"ë¡œ ë³€ê²½ í™•ì¸

### âœ… Phase 4 ì™„ë£Œ ì¡°ê±´

- [ ] Frontend í™”ë©´ì— ë°°ì¹˜ ëª©ë¡ í‘œì‹œ
- [ ] "ìƒˆ ë°°ì¹˜ ì‹œì‘" ë²„íŠ¼ ë™ì‘
- [ ] "ì¤‘ì§€" ë²„íŠ¼ ë™ì‘
- [ ] 2ì´ˆë§ˆë‹¤ ìë™ ê°±ì‹ 

---

## Phase 5: ì‹œê°í™” êµ¬í˜„

### ğŸ¯ ëª©í‘œ
íŒŒí‹°í´ ì‹œìŠ¤í…œ + ì• ë‹ˆë©”ì´ì…˜

### ğŸ“š ì°¸ì¡° ë¬¸ì„œ
**ANIMATION_SPEC.md** (31KB)ë¥¼ ì½ì–´ì£¼ì„¸ìš”.

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] Three.js 3D ë°°ê²½
- [ ] Konva íŒŒí‹°í´ ì‹œìŠ¤í…œ
- [ ] Web Worker
- [ ] ì• ë‹ˆë©”ì´ì…˜ íƒ€ì„ë¼ì¸
- [ ] í…ŒìŠ¤íŠ¸

### ğŸš€ ì‹¤í–‰

**ANIMATION_SPEC.mdì˜ ì½”ë“œë¥¼ ë‹¨ê³„ë³„ë¡œ ë³µì‚¬-ë¶™ì—¬ë„£ê¸°**

ì´ PhaseëŠ” ë§¤ìš° í¬ë¯€ë¡œ, ANIMATION_SPEC.mdë¥¼ ì°¸ì¡°í•˜ë©´ì„œ ì§„í–‰í•˜ì„¸ìš”.

---

## âœ… ì „ì²´ ì™„ë£Œ í™•ì¸

### ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Phase 1: í”„ë¡œì íŠ¸ ìƒì„± ì™„ë£Œ
- [ ] Phase 2: Database 8ê°œ í…Œì´ë¸” ìƒì„±
- [ ] Phase 3: Backend API ë™ì‘
- [ ] Phase 4: Frontend ê¸°ë³¸ ë™ì‘
- [ ] Phase 5: ì‹œê°í™” êµ¬í˜„

### ì ‘ì† í™•ì¸

```bash
# Backend
curl http://localhost:8001/health
curl http://localhost:8001/api/control/batches

# Frontend
open http://localhost:5174

# Database
psql -h localhost -p 5433 -U visualizer_admin -d visualizer -c "\dt"
```

---

## ğŸ‰ ì™„ë£Œ!

ëª¨ë“  Phaseë¥¼ ì™„ë£Œí–ˆë‹¤ë©´, ì´ì œ AI Visualizerê°€ ë™ì‘í•©ë‹ˆë‹¤!

ë‹¤ìŒ ë‹¨ê³„:
1. ì‹¤ì œ ë°ì´í„° ì—°ë™
2. ì• ë‹ˆë©”ì´ì…˜ ìµœì í™”
3. ê´€ì œ ì‹œìŠ¤í…œ êµ¬í˜„

**Happy Coding! ğŸš€**

---

**ì‘ì„±ì¼**: 2025-12-08
**ì‘ì„±ì**: wonny (for Claude Sonnet)
**ë²„ì „**: 1.0.0
